# Several open source LLM evaluation tools

[Promptfoo](https://github.com/promptfoo/promptfoo)  —  CLI for prompt testing, red-teaming, and model comparison

DeepEval  —  Pytest-style framework with 50+ LLM metrics for RAG
▸github.com/confident-ai/deepeval
Ragas  —  RAG evaluation with auto test dataset generation
▸github.com/explodinggradients/ragas
OpenAI Evals  —  Benchmark registry with automated and human grading
▸github.com/openai/evals
Comet Opik  —  Tracing, evaluation, and monitoring for LLM agents
▸github.com/comet-ml/opik
