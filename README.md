# Several open source LLM evaluation tools

[Promptfoo](https://github.com/promptfoo/promptfoo)  —  CLI for prompt testing, red-teaming, and model comparison

[DeepEval](https://github.com/confident-ai/deepeval)  —  Pytest-style framework with 50+ LLM metrics for RAG

[Ragas](https://github.com/explodinggradients/ragas)  —  RAG evaluation with auto test dataset generation

[OpenAI Evals](https://github.com/openai/evals)  —  Benchmark registry with automated and human grading

[Comet Opik](https://github.com/comet-ml/opik)  —  Tracing, evaluation, and monitoring for LLM agents

